{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 205
    },
    "colab_type": "code",
    "id": "7aX_oKDWojdd",
    "outputId": "5b4f738a-1a11-4a50-de8b-0f38ebb7c5af"
   },
   "source": [
    "# <span style=\"color:#0b486b\">  Gender Classification Based on Tweets</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">Sebastian Guerra </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this project is to develop an algorithm capable of predicting the people's gender based on tweets. For the first part, NLP is used in order to clean and obtain the important words. Libraries used:\n",
    "<br><br>-Gensim<br>\n",
    "-Spacy<br>\n",
    "-Pandas<br>\n",
    "-Emoji<br>\n",
    "\n",
    "Later, machine learning algorithms were developed to try to achieve the highest accuracy. Withing the models used are:<br>\n",
    "\n",
    "-RandomForest<br>\n",
    "-SVC<br>\n",
    "-A mix of Doc2Vec with SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Packages required to running this file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "colab_type": "code",
    "id": "yiz_KSKpGSr8",
    "outputId": "793f8b92-d55e-4697-cccd-8403ec8e47ea"
   },
   "outputs": [],
   "source": [
    "# !pip install emoji\n",
    "# !pip install spacy\n",
    "# !pip install pandas\n",
    "# !pip install gensim\n",
    "# !pip install scikit-learn\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following libraries are used for this Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D-T0_MMZcSuH"
   },
   "outputs": [],
   "source": [
    "# for Preprocessing\n",
    "import os\n",
    "import csv\n",
    "import xml.etree.ElementTree as ET\n",
    "from itertools import chain\n",
    "import spacy\n",
    "import re\n",
    "import pickle\n",
    "import emoji\n",
    "from emoji.unicode_codes import UNICODE_EMOJI\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "\n",
    "#Gensim package (for Document Vectorization and Bigram generation)\n",
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.models import Phrases\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "#Sklearn for model building\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sg1sSw05c4xp"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "path = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bSVEzfCHHUJy"
   },
   "outputs": [],
   "source": [
    "# !unzip \"data.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the tweets from the `XML` files are extracted along with the author ID and are added to a tuple for further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZT4psVIhcNbe",
    "outputId": "e52e255b-9f26-478b-cc70-f5b07340ee87"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Document Parsing: 100%|████████████████████████████████████████████████████████████| 3601/3601 [01:09<00:00, 51.97it/s]\n"
     ]
    }
   ],
   "source": [
    "tokens_author_dict = {}\n",
    "documents_data = []\n",
    "for file_name in tqdm(os.listdir(path), desc = \"Document Parsing\"): # Iterates through all files present in the data directory\n",
    "    if \".xml\" in file_name:\n",
    "        root = ET.parse('data/' + file_name).getroot()\n",
    "\n",
    "        post_sentences = [] # Extract the all the post sentences and append it to the the list\n",
    "        for type_tag in root.findall('documents/document'):\n",
    "            post_sentences.append(type_tag.text)\n",
    "\n",
    "        corpus = \" \".join(post_sentences) # merges all the tweets for a person into single document\n",
    "        auth_id = file_name.replace('.xml', '')\n",
    "        documents_data.append((auth_id, corpus)) # the document and its corresponding author id is added to the final list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data frame containing list of all Authors along with the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qv7v6SQ5uWQ8"
   },
   "outputs": [],
   "source": [
    "documents_data = pd.DataFrame(data=documents_data, columns=[\"Author_ID\", \"Documents\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Preprocessing** function accepts the document`(contains all the tweets for a single user)` for each user and cleans the\n",
    "`urls`, `html tags`, `twitter users`, `puntuations`, `stopwords`, `digits`, `brackets` and `currency` symbols if there are any present in the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CRdtXBbVzqWS"
   },
   "outputs": [],
   "source": [
    "url_regex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
    "def Preprocessing(data):\n",
    "    \"\"\"\n",
    "    reads the documents and apply all the necessary preprocessing and finally returns the lemmatized tokens\n",
    "    \"\"\"\n",
    "    doc = re.sub(\"[\\n]*\", \"\", data, flags=re.MULTILINE) #filters newlines\n",
    "    doc = re.sub(\"@[\\w]*\", \"\", doc) # filter all the users\n",
    "    doc  = re.sub(\"&nbsp;| &amp;| &lt; | &gt;| - \", \"\", doc) # filters all html tags\n",
    "    doc = re.sub(\"✔\", \"\", doc) # filters the verified user symbol\n",
    "    doc = re.sub(url_regex, \"\", doc) # filters the url\n",
    "    doc = nlp(doc)\n",
    "    filtered_tokens = []\n",
    "    for token in doc: # spacy tokenizer is used here to do tokenization as well as the following preprocessing\n",
    "        if not (token.is_stop | token.is_punct | token.is_bracket | token.is_digit | token.is_currency):\n",
    "            filtered_tokens.append(token.lemma_) # lemmatizes the final tokens before adding them to the final list\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **postprocessing** function filters all the tokens which have  size less than 3 as these tokens doesn't add any meaning to the classification task and the whitespaces created during preprocessing stage were removed during the stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MReuZM-zVfut"
   },
   "outputs": [],
   "source": [
    "def postprocessing(row):\n",
    "    \"\"\"\n",
    "    filters the tokens with size less than 3 and removes the extra white spaces\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for token in row:\n",
    "        if len(token)<3: # filters the less token with size less than 3\n",
    "            continue\n",
    "        if re.match(\"[\\s]+\", token): # filters white spaces\n",
    "            continue\n",
    "        result.append(token)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "applies the prep and post processing functions to the `documents`. all the unicode characters i.e.(emojis) were also filtered from the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AZ5W-AP3LRJA"
   },
   "outputs": [],
   "source": [
    "documents_data[\"Documents\"] = documents_data[\"Documents\"].apply(lambda x: Preprocessing(x))\n",
    "documents_data[\"Documents\"] = documents_data[\"Documents\"].apply(lambda x: [ \"\" if (token in UNICODE_EMOJI) else token for token in x])\n",
    "documents_data[\"Documents\"] = documents_data[\"Documents\"].apply(lambda x: postprocessing(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data is extracted from the Document corpus based on the `Author_ID`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ovabFB6bgYFr"
   },
   "outputs": [],
   "source": [
    "train_data = pd.merge(documents_data, train_labels, left_on=\"Author_ID\", right_on=\"id\", copy=False).drop(labels = [\"Author_ID\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_tag =  {\"male\":1, \"female\":0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"test_labels.csv\")\n",
    "test_data = pd.merge(documents_data, test, left_on=\"Author_ID\", right_on=\"id\", copy=False).drop(labels = [\"Author_ID\"], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first model which we have created is based on the combination of TF-IDF and Random Forest Classifier\n",
    "\n",
    "\n",
    "Creating a Tfidf Vectorizer to convert all tokens into a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GralRRBx0ykO"
   },
   "outputs": [],
   "source": [
    "# Initializing the TfidfVectorizer to disregard words more frequent than 95% and lesser than count 5 and also with bigrams and trigrams.\n",
    "vectorizer = TfidfVectorizer(decode_error=\"ignore\", \n",
    "                             max_df = 0.95, \n",
    "                             min_df = 5,\n",
    "                             max_features=1000,\n",
    "                             token_pattern=None,\n",
    "                             tokenizer = lambda x: x,\n",
    "                             preprocessor = lambda x: x,\n",
    "                             ngram_range=(1,3)\n",
    "                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing Training and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eKyqsMwsbJyF"
   },
   "outputs": [],
   "source": [
    "# Fitting the tfidf vectorizer over the training documents\n",
    "X = vectorizer.fit_transform(train_data.Documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = vectorizer.transform(test_data.Documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first model that we're training is a RandomForestClassifier. In order to find the optimal parameters for this classifier model, we use grid search ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing Gridsearch for Randomforest\n",
    "param_grid = {'n_estimators':[3500, 4000, 5000, 6000],'criterion':['gini']}\n",
    "\n",
    "gridVectorizer = GridSearchCV(RandomForestClassifier(),param_grid,refit = True, verbose=2, cv=2)\n",
    "\n",
    "gridVectorizer.fit(X, train_data.gender.map(result_tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'criterion': 'gini', 'n_estimators': 3500}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above optimal parameters, we then train a randomforest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing a Random Forest model with the optimal values found after running a gridsearchcv.\n",
    "clf = RandomForestClassifier(bootstrap=True, n_estimators=3500)\n",
    "# clf = pickle.load(open(\"finalized_rf_tfidf_model.sav\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the training dataset on the randomforest model\n",
    "clf.fit(X, train_data.gender.map(result_tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Test accuracy for the Random Forest Classifer Model is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.768"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, test_data.gender.map(result_tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second model which we have created is based on the combination of TF-IDF and SVC\n",
    "\n",
    "\n",
    "Next, we decide to attempt SVC model. In order to find the optimal value, we run a grid search on the SVC model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing Gridsearch for SVC\n",
    "\n",
    "param_grid = {'C':[ 0.5, 1, 2,10],'degree':[1, 2, 3, 10], 'kernel':['linear','rbf']}\n",
    "\n",
    "gridSVCVectorizer = GridSearchCV(SVC(),param_grid,refit = True, verbose=2, cv=2)\n",
    "\n",
    "gridSVCVectorizer.fit(X, train_data.gender.map(result_tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Best Parameters We have got from the above run is\n",
    "\n",
    "`C`: **1**, \n",
    "\n",
    "`degree`: **1**, \n",
    "\n",
    "`kernel`: **rbf**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above optimal parameters, we then train a SVC model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 27507,
     "status": "ok",
     "timestamp": 1591191762674,
     "user": {
      "displayName": "Krishna Bikki",
      "photoUrl": "",
      "userId": "14890876778829126825"
     },
     "user_tz": -600
    },
    "id": "1I5nH1O7bgbS",
    "outputId": "1e88651a-e553-4ef2-f236-aa4fa355f608"
   },
   "outputs": [],
   "source": [
    "# Initializing a SVC model with the optimal values found after running a gridsearchcv.\n",
    "model_svc = SVC(kernel=\"rbf\", C=1, degree=1)\n",
    "# model_svc = pickle.load(open(\"svc_tfidf_model.sav\", 'rb'))\n",
    "model_svc.fit(X, train_data.gender.map(result_tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_svc.score(X_test, test_data.gender.map(result_tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of the 2 models `(Random Forest Classifier & Support Vector Classifier)` which were built on top of TF-IDF the second model gave us the best results. i.e. The combination of **TF-IDF and SVC**. But since the improvement was not significant, we began exploring our options in how to vectorize the documents differently in order to generate better trends.\n",
    "\n",
    "In order to further improve the classification accuracy we have used the below model using Doc2Vec vectorizer in the gensim package. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third model\n",
    "\n",
    "#### The Third model which we have created is based on the combination of Doc2Vec and SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Since for the above two models we have used TF-IDF inbuilt n-grams for the model, in this method we have used gensim `Phrases`\n",
    "* The `Phrases` will adds the bigrams to the documents corpus for each document iteratively \n",
    "* The min_count parameter is used to filter less frequent bigrams and tokens from the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "8HT9d34Ldl_f",
    "outputId": "84a15340-c42c-4f8e-929d-64b1bea17799"
   },
   "outputs": [],
   "source": [
    "bigram = Phrases(documents_data.Documents, min_count=5) # creates bigrams from the Document Corpus\n",
    "for idx in range(len(documents_data.Documents)): # iterates over all the documents\n",
    "    for token in bigram[documents_data.Documents[idx]]: \n",
    "        if '_' in token: # if the token is a bigram it will add it back to the respective document from which it was generated\n",
    "            documents_data.Documents[idx].append(token) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the cleaned documents were saved to a file for further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yg9QJXExNK7L"
   },
   "outputs": [],
   "source": [
    "documents_data.to_csv(\"cleaned_Documents.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4rzduFJ2or9A"
   },
   "outputs": [],
   "source": [
    "train_labels = pd.read_csv(\"train_labels.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data is extracted from the Document corpus based on the `Author_ID`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ovabFB6bgYFr"
   },
   "outputs": [],
   "source": [
    "train_data = pd.merge(documents_data, train_labels, left_on=\"Author_ID\", right_on=\"id\", copy=False).drop(labels = [\"Author_ID\"], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Doc2Vec_Model class is an adapter for gensim Doc2vec for the compatability with scikitlearn GridsearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 36 candidates, totalling 72 fits\n",
      "[CV] doc2vec__vector_size=300, doc2vec__window=3, svc__C=0.3, svc__kernel=linear \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  doc2vec__vector_size=300, doc2vec__window=3, svc__C=0.3, svc__kernel=linear, score=0.793, total= 2.2min\n",
      "[CV] doc2vec__vector_size=300, doc2vec__window=3, svc__C=0.3, svc__kernel=linear \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.2min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  doc2vec__vector_size=300, doc2vec__window=3, svc__C=0.3, svc__kernel=linear, score=0.775, total= 2.2min\n",
      "[CV] doc2vec__vector_size=300, doc2vec__window=3, svc__C=0.3, svc__kernel=rbf \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  4.4min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  doc2vec__vector_size=300, doc2vec__window=3, svc__C=0.3, svc__kernel=rbf, score=0.797, total= 2.2min\n",
      "[CV] doc2vec__vector_size=300, doc2vec__window=3, svc__C=0.3, svc__kernel=rbf \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  6.6min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  doc2vec__vector_size=300, doc2vec__window=3, svc__C=0.3, svc__kernel=rbf, score=0.778, total= 2.2min\n",
      "[CV] doc2vec__vector_size=300, doc2vec__window=3, svc__C=0.6, svc__kernel=linear \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:  8.8min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  doc2vec__vector_size=300, doc2vec__window=3, svc__C=0.6, svc__kernel=linear, score=0.786, total= 2.3min\n",
      "[CV] doc2vec__vector_size=300, doc2vec__window=3, svc__C=0.6, svc__kernel=linear \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 11.1min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  doc2vec__vector_size=300, doc2vec__window=3, svc__C=0.6, svc__kernel=linear, score=0.771, total= 2.3min\n",
      "[CV] doc2vec__vector_size=300, doc2vec__window=3, svc__C=0.6, svc__kernel=rbf \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed: 13.4min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  doc2vec__vector_size=300, doc2vec__window=3, svc__C=0.6, svc__kernel=rbf, score=0.793, total= 2.3min\n",
      "[CV] doc2vec__vector_size=300, doc2vec__window=3, svc__C=0.6, svc__kernel=rbf \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed: 15.7min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  doc2vec__vector_size=300, doc2vec__window=3, svc__C=0.6, svc__kernel=rbf, score=0.783, total= 2.3min\n",
      "[CV] doc2vec__vector_size=300, doc2vec__window=3, svc__C=0.9, svc__kernel=linear \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed: 18.0min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  doc2vec__vector_size=300, doc2vec__window=3, svc__C=0.9, svc__kernel=linear, score=0.787, total= 2.3min\n",
      "[CV] doc2vec__vector_size=300, doc2vec__window=3, svc__C=0.9, svc__kernel=linear \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed: 20.3min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  doc2vec__vector_size=300, doc2vec__window=3, svc__C=0.9, svc__kernel=linear, score=0.766, total= 2.3min\n",
      "[CV] doc2vec__vector_size=300, doc2vec__window=3, svc__C=0.9, svc__kernel=rbf \n",
      "[CV]  doc2vec__vector_size=300, doc2vec__window=3, svc__C=0.9, svc__kernel=rbf, score=0.799, total= 2.2min\n",
      "[CV] doc2vec__vector_size=300, doc2vec__window=3, svc__C=0.9, svc__kernel=rbf \n",
      "[CV]  doc2vec__vector_size=300, doc2vec__window=3, svc__C=0.9, svc__kernel=rbf, score=0.778, total= 2.3min\n",
      "[CV] doc2vec__vector_size=300, doc2vec__window=6, svc__C=0.3, svc__kernel=linear \n",
      "[CV]  doc2vec__vector_size=300, doc2vec__window=6, svc__C=0.3, svc__kernel=linear, score=0.795, total= 2.3min\n",
      "[CV] doc2vec__vector_size=300, doc2vec__window=6, svc__C=0.3, svc__kernel=linear \n",
      "[CV]  doc2vec__vector_size=300, doc2vec__window=6, svc__C=0.3, svc__kernel=linear, score=0.769, total= 2.3min\n",
      "[CV] doc2vec__vector_size=300, doc2vec__window=6, svc__C=0.3, svc__kernel=rbf \n",
      "[CV]  doc2vec__vector_size=300, doc2vec__window=6, svc__C=0.3, svc__kernel=rbf, score=0.797, total= 2.3min\n",
      "[CV] doc2vec__vector_size=300, doc2vec__window=6, svc__C=0.3, svc__kernel=rbf \n",
      "[CV]  doc2vec__vector_size=300, doc2vec__window=6, svc__C=0.3, svc__kernel=rbf, score=0.790, total= 2.3min\n",
      "[CV] doc2vec__vector_size=300, doc2vec__window=6, svc__C=0.6, svc__kernel=linear \n",
      "[CV]  doc2vec__vector_size=300, doc2vec__window=6, svc__C=0.6, svc__kernel=linear, score=0.794, total= 2.3min\n",
      "[CV] doc2vec__vector_size=300, doc2vec__window=6, svc__C=0.6, svc__kernel=linear \n",
      "[CV]  doc2vec__vector_size=300, doc2vec__window=6, svc__C=0.6, svc__kernel=linear, score=0.776, total= 2.3min\n",
      "[CV] doc2vec__vector_size=300, doc2vec__window=6, svc__C=0.6, svc__kernel=rbf \n",
      "[CV]  doc2vec__vector_size=300, doc2vec__window=6, svc__C=0.6, svc__kernel=rbf, score=0.798, total= 2.3min\n",
      "[CV] doc2vec__vector_size=300, doc2vec__window=6, svc__C=0.6, svc__kernel=rbf \n",
      "[CV]  doc2vec__vector_size=300, doc2vec__window=6, svc__C=0.6, svc__kernel=rbf, score=0.782, total= 2.3min\n",
      "[CV] doc2vec__vector_size=300, doc2vec__window=6, svc__C=0.9, svc__kernel=linear \n",
      "[CV]  doc2vec__vector_size=300, doc2vec__window=6, svc__C=0.9, svc__kernel=linear, score=0.786, total= 2.4min\n",
      "[CV] doc2vec__vector_size=300, doc2vec__window=6, svc__C=0.9, svc__kernel=linear \n",
      "[CV]  doc2vec__vector_size=300, doc2vec__window=6, svc__C=0.9, svc__kernel=linear, score=0.776, total= 2.2min\n",
      "[CV] doc2vec__vector_size=300, doc2vec__window=6, svc__C=0.9, svc__kernel=rbf \n",
      "[CV]  doc2vec__vector_size=300, doc2vec__window=6, svc__C=0.9, svc__kernel=rbf, score=0.803, total= 2.2min\n",
      "[CV] doc2vec__vector_size=300, doc2vec__window=6, svc__C=0.9, svc__kernel=rbf \n",
      "[CV]  doc2vec__vector_size=300, doc2vec__window=6, svc__C=0.9, svc__kernel=rbf, score=0.788, total= 2.2min\n",
      "[CV] doc2vec__vector_size=300, doc2vec__window=9, svc__C=0.3, svc__kernel=linear \n",
      "[CV]  doc2vec__vector_size=300, doc2vec__window=9, svc__C=0.3, svc__kernel=linear, score=0.790, total= 2.2min\n",
      "[CV] doc2vec__vector_size=300, doc2vec__window=9, svc__C=0.3, svc__kernel=linear \n",
      "[CV]  doc2vec__vector_size=300, doc2vec__window=9, svc__C=0.3, svc__kernel=linear, score=0.764, total= 2.2min\n",
      "[CV] doc2vec__vector_size=300, doc2vec__window=9, svc__C=0.3, svc__kernel=rbf \n",
      "[CV]  doc2vec__vector_size=300, doc2vec__window=9, svc__C=0.3, svc__kernel=rbf, score=0.790, total= 2.2min\n",
      "[CV] doc2vec__vector_size=300, doc2vec__window=9, svc__C=0.3, svc__kernel=rbf \n",
      "[CV]  doc2vec__vector_size=300, doc2vec__window=9, svc__C=0.3, svc__kernel=rbf, score=0.783, total= 2.3min\n",
      "[CV] doc2vec__vector_size=300, doc2vec__window=9, svc__C=0.6, svc__kernel=linear \n",
      "[CV]  doc2vec__vector_size=300, doc2vec__window=9, svc__C=0.6, svc__kernel=linear, score=0.793, total= 2.3min\n",
      "[CV] doc2vec__vector_size=300, doc2vec__window=9, svc__C=0.6, svc__kernel=linear \n",
      "[CV]  doc2vec__vector_size=300, doc2vec__window=9, svc__C=0.6, svc__kernel=linear, score=0.761, total= 2.4min\n",
      "[CV] doc2vec__vector_size=300, doc2vec__window=9, svc__C=0.6, svc__kernel=rbf \n",
      "[CV]  doc2vec__vector_size=300, doc2vec__window=9, svc__C=0.6, svc__kernel=rbf, score=0.797, total= 2.3min\n",
      "[CV] doc2vec__vector_size=300, doc2vec__window=9, svc__C=0.6, svc__kernel=rbf \n",
      "[CV]  doc2vec__vector_size=300, doc2vec__window=9, svc__C=0.6, svc__kernel=rbf, score=0.782, total= 2.2min\n",
      "[CV] doc2vec__vector_size=300, doc2vec__window=9, svc__C=0.9, svc__kernel=linear \n",
      "[CV]  doc2vec__vector_size=300, doc2vec__window=9, svc__C=0.9, svc__kernel=linear, score=0.786, total= 2.3min\n",
      "[CV] doc2vec__vector_size=300, doc2vec__window=9, svc__C=0.9, svc__kernel=linear \n",
      "[CV]  doc2vec__vector_size=300, doc2vec__window=9, svc__C=0.9, svc__kernel=linear, score=0.771, total= 2.2min\n",
      "[CV] doc2vec__vector_size=300, doc2vec__window=9, svc__C=0.9, svc__kernel=rbf \n",
      "[CV]  doc2vec__vector_size=300, doc2vec__window=9, svc__C=0.9, svc__kernel=rbf, score=0.799, total= 2.2min\n",
      "[CV] doc2vec__vector_size=300, doc2vec__window=9, svc__C=0.9, svc__kernel=rbf \n",
      "[CV]  doc2vec__vector_size=300, doc2vec__window=9, svc__C=0.9, svc__kernel=rbf, score=0.785, total= 2.2min\n",
      "[CV] doc2vec__vector_size=500, doc2vec__window=3, svc__C=0.3, svc__kernel=linear \n",
      "[CV]  doc2vec__vector_size=500, doc2vec__window=3, svc__C=0.3, svc__kernel=linear, score=0.801, total= 3.0min\n",
      "[CV] doc2vec__vector_size=500, doc2vec__window=3, svc__C=0.3, svc__kernel=linear \n",
      "[CV]  doc2vec__vector_size=500, doc2vec__window=3, svc__C=0.3, svc__kernel=linear, score=0.772, total= 3.0min\n",
      "[CV] doc2vec__vector_size=500, doc2vec__window=3, svc__C=0.3, svc__kernel=rbf \n",
      "[CV]  doc2vec__vector_size=500, doc2vec__window=3, svc__C=0.3, svc__kernel=rbf, score=0.799, total= 3.0min\n",
      "[CV] doc2vec__vector_size=500, doc2vec__window=3, svc__C=0.3, svc__kernel=rbf \n",
      "[CV]  doc2vec__vector_size=500, doc2vec__window=3, svc__C=0.3, svc__kernel=rbf, score=0.787, total= 3.0min\n",
      "[CV] doc2vec__vector_size=500, doc2vec__window=3, svc__C=0.6, svc__kernel=linear \n",
      "[CV]  doc2vec__vector_size=500, doc2vec__window=3, svc__C=0.6, svc__kernel=linear, score=0.790, total= 3.0min\n",
      "[CV] doc2vec__vector_size=500, doc2vec__window=3, svc__C=0.6, svc__kernel=linear \n",
      "[CV]  doc2vec__vector_size=500, doc2vec__window=3, svc__C=0.6, svc__kernel=linear, score=0.763, total= 3.0min\n",
      "[CV] doc2vec__vector_size=500, doc2vec__window=3, svc__C=0.6, svc__kernel=rbf \n",
      "[CV]  doc2vec__vector_size=500, doc2vec__window=3, svc__C=0.6, svc__kernel=rbf, score=0.807, total= 3.0min\n",
      "[CV] doc2vec__vector_size=500, doc2vec__window=3, svc__C=0.6, svc__kernel=rbf \n",
      "[CV]  doc2vec__vector_size=500, doc2vec__window=3, svc__C=0.6, svc__kernel=rbf, score=0.790, total= 3.0min\n",
      "[CV] doc2vec__vector_size=500, doc2vec__window=3, svc__C=0.9, svc__kernel=linear \n",
      "[CV]  doc2vec__vector_size=500, doc2vec__window=3, svc__C=0.9, svc__kernel=linear, score=0.795, total= 3.0min\n",
      "[CV] doc2vec__vector_size=500, doc2vec__window=3, svc__C=0.9, svc__kernel=linear \n",
      "[CV]  doc2vec__vector_size=500, doc2vec__window=3, svc__C=0.9, svc__kernel=linear, score=0.768, total= 3.0min\n",
      "[CV] doc2vec__vector_size=500, doc2vec__window=3, svc__C=0.9, svc__kernel=rbf \n",
      "[CV]  doc2vec__vector_size=500, doc2vec__window=3, svc__C=0.9, svc__kernel=rbf, score=0.797, total= 3.0min\n",
      "[CV] doc2vec__vector_size=500, doc2vec__window=3, svc__C=0.9, svc__kernel=rbf \n",
      "[CV]  doc2vec__vector_size=500, doc2vec__window=3, svc__C=0.9, svc__kernel=rbf, score=0.787, total= 3.0min\n",
      "[CV] doc2vec__vector_size=500, doc2vec__window=6, svc__C=0.3, svc__kernel=linear \n",
      "[CV]  doc2vec__vector_size=500, doc2vec__window=6, svc__C=0.3, svc__kernel=linear, score=0.804, total= 3.3min\n",
      "[CV] doc2vec__vector_size=500, doc2vec__window=6, svc__C=0.3, svc__kernel=linear \n",
      "[CV]  doc2vec__vector_size=500, doc2vec__window=6, svc__C=0.3, svc__kernel=linear, score=0.767, total= 3.0min\n",
      "[CV] doc2vec__vector_size=500, doc2vec__window=6, svc__C=0.3, svc__kernel=rbf \n",
      "[CV]  doc2vec__vector_size=500, doc2vec__window=6, svc__C=0.3, svc__kernel=rbf, score=0.800, total= 3.0min\n",
      "[CV] doc2vec__vector_size=500, doc2vec__window=6, svc__C=0.3, svc__kernel=rbf \n",
      "[CV]  doc2vec__vector_size=500, doc2vec__window=6, svc__C=0.3, svc__kernel=rbf, score=0.782, total= 3.0min\n",
      "[CV] doc2vec__vector_size=500, doc2vec__window=6, svc__C=0.6, svc__kernel=linear \n",
      "[CV]  doc2vec__vector_size=500, doc2vec__window=6, svc__C=0.6, svc__kernel=linear, score=0.792, total= 3.0min\n",
      "[CV] doc2vec__vector_size=500, doc2vec__window=6, svc__C=0.6, svc__kernel=linear \n",
      "[CV]  doc2vec__vector_size=500, doc2vec__window=6, svc__C=0.6, svc__kernel=linear, score=0.770, total= 3.0min\n",
      "[CV] doc2vec__vector_size=500, doc2vec__window=6, svc__C=0.6, svc__kernel=rbf \n",
      "[CV]  doc2vec__vector_size=500, doc2vec__window=6, svc__C=0.6, svc__kernel=rbf, score=0.801, total= 3.0min\n",
      "[CV] doc2vec__vector_size=500, doc2vec__window=6, svc__C=0.6, svc__kernel=rbf \n",
      "[CV]  doc2vec__vector_size=500, doc2vec__window=6, svc__C=0.6, svc__kernel=rbf, score=0.782, total= 3.0min\n",
      "[CV] doc2vec__vector_size=500, doc2vec__window=6, svc__C=0.9, svc__kernel=linear \n",
      "[CV]  doc2vec__vector_size=500, doc2vec__window=6, svc__C=0.9, svc__kernel=linear, score=0.789, total= 2.9min\n",
      "[CV] doc2vec__vector_size=500, doc2vec__window=6, svc__C=0.9, svc__kernel=linear \n",
      "[CV]  doc2vec__vector_size=500, doc2vec__window=6, svc__C=0.9, svc__kernel=linear, score=0.773, total= 3.0min\n",
      "[CV] doc2vec__vector_size=500, doc2vec__window=6, svc__C=0.9, svc__kernel=rbf \n",
      "[CV]  doc2vec__vector_size=500, doc2vec__window=6, svc__C=0.9, svc__kernel=rbf, score=0.807, total= 3.0min\n",
      "[CV] doc2vec__vector_size=500, doc2vec__window=6, svc__C=0.9, svc__kernel=rbf \n",
      "[CV]  doc2vec__vector_size=500, doc2vec__window=6, svc__C=0.9, svc__kernel=rbf, score=0.779, total= 9.6min\n",
      "[CV] doc2vec__vector_size=500, doc2vec__window=9, svc__C=0.3, svc__kernel=linear \n",
      "[CV]  doc2vec__vector_size=500, doc2vec__window=9, svc__C=0.3, svc__kernel=linear, score=0.803, total= 3.0min\n",
      "[CV] doc2vec__vector_size=500, doc2vec__window=9, svc__C=0.3, svc__kernel=linear \n",
      "[CV]  doc2vec__vector_size=500, doc2vec__window=9, svc__C=0.3, svc__kernel=linear, score=0.757, total= 3.1min\n",
      "[CV] doc2vec__vector_size=500, doc2vec__window=9, svc__C=0.3, svc__kernel=rbf \n",
      "[CV]  doc2vec__vector_size=500, doc2vec__window=9, svc__C=0.3, svc__kernel=rbf, score=0.805, total= 3.1min\n",
      "[CV] doc2vec__vector_size=500, doc2vec__window=9, svc__C=0.3, svc__kernel=rbf \n",
      "[CV]  doc2vec__vector_size=500, doc2vec__window=9, svc__C=0.3, svc__kernel=rbf, score=0.787, total= 3.1min\n",
      "[CV] doc2vec__vector_size=500, doc2vec__window=9, svc__C=0.6, svc__kernel=linear \n",
      "[CV]  doc2vec__vector_size=500, doc2vec__window=9, svc__C=0.6, svc__kernel=linear, score=0.795, total= 3.1min\n",
      "[CV] doc2vec__vector_size=500, doc2vec__window=9, svc__C=0.6, svc__kernel=linear \n",
      "[CV]  doc2vec__vector_size=500, doc2vec__window=9, svc__C=0.6, svc__kernel=linear, score=0.768, total= 3.1min\n",
      "[CV] doc2vec__vector_size=500, doc2vec__window=9, svc__C=0.6, svc__kernel=rbf \n",
      "[CV]  doc2vec__vector_size=500, doc2vec__window=9, svc__C=0.6, svc__kernel=rbf, score=0.795, total= 3.1min\n",
      "[CV] doc2vec__vector_size=500, doc2vec__window=9, svc__C=0.6, svc__kernel=rbf \n",
      "[CV]  doc2vec__vector_size=500, doc2vec__window=9, svc__C=0.6, svc__kernel=rbf, score=0.782, total= 3.1min\n",
      "[CV] doc2vec__vector_size=500, doc2vec__window=9, svc__C=0.9, svc__kernel=linear \n",
      "[CV]  doc2vec__vector_size=500, doc2vec__window=9, svc__C=0.9, svc__kernel=linear, score=0.785, total= 3.0min\n",
      "[CV] doc2vec__vector_size=500, doc2vec__window=9, svc__C=0.9, svc__kernel=linear \n",
      "[CV]  doc2vec__vector_size=500, doc2vec__window=9, svc__C=0.9, svc__kernel=linear, score=0.766, total= 3.0min\n",
      "[CV] doc2vec__vector_size=500, doc2vec__window=9, svc__C=0.9, svc__kernel=rbf \n",
      "[CV]  doc2vec__vector_size=500, doc2vec__window=9, svc__C=0.9, svc__kernel=rbf, score=0.798, total= 3.1min\n",
      "[CV] doc2vec__vector_size=500, doc2vec__window=9, svc__C=0.9, svc__kernel=rbf \n",
      "[CV]  doc2vec__vector_size=500, doc2vec__window=9, svc__C=0.9, svc__kernel=rbf, score=0.783, total= 3.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  72 out of  72 | elapsed: 196.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'doc2vec__vector_size': 500, 'doc2vec__window': 3, 'svc__C': 0.6, 'svc__kernel': 'rbf'}\n",
      "\n",
      "Best accuracy: 0.7987096774193548\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Doc2Vec_Model(BaseEstimator):\n",
    "    \"\"\"\n",
    "    Since gensim Doc2vec model is not directly compatible to run on sckitlearn GridSearchCV. The Doc2Vec_Model class acts as an adapter for the\n",
    "    gensim Doc2vec for compatibility. Scikitlearn learn BaseEstimator is subclassed to\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vector_size = 100, window = 5):\n",
    "        \"\"\"\n",
    "        In the init we are Initializing all the parameters required by the Doc2vec model \n",
    "        \"\"\"\n",
    "        self.epoch = 10 # number of epochs the doc2vec will train on the input data\n",
    "        self.vector_size = vector_size # dimension of the output vector\n",
    "        self.hs= 1 # for softmax output\n",
    "        self.window = window # window size of doc2vec \n",
    "        self.dm = 0 #Paragraph Vector - Distributed Bag of Words\n",
    "\n",
    "    def fit(self, tagged_documents, y=None):\n",
    "        \"\"\"\n",
    "        This function is used to train the Doc2vec model on the input data\n",
    "        \"\"\"\n",
    "        \n",
    "        # Initialize model\n",
    "        self.d2v_model = Doc2Vec(dm=self.dm, vector_size = self.vector_size, window = self.window, hs = self.hs, alpha=0.025, min_alpha=0.001)\n",
    "        \n",
    "        # Build vocabulary\n",
    "        self.d2v_model.build_vocab([x for x in tagged_documents])\n",
    "        # Model training\n",
    "        self.d2v_model.train(tagged_documents, total_examples=len(tagged_documents), epochs=self.epoch)\n",
    "        return self\n",
    "\n",
    "    def transform(self, data):\n",
    "        \"\"\"\n",
    "        This function will transform the input documents into their corresponding vectors\n",
    "        \"\"\"\n",
    "        # the model infer_vector function will return the document vector for the input data \n",
    "        _, vector = zip(*[(doc.tags[0], self.d2v_model.infer_vector(doc.words, steps=20)) for doc in data])\n",
    "        return  vector\n",
    "\n",
    "    def fit_transform(self, docs, y=None):\n",
    "        \"\"\"\n",
    "        This function will run both the fit and transform steps and returns the transformed data\n",
    "        \"\"\"\n",
    "        self.fit(docs)\n",
    "        return self.transform(docs)\n",
    "\n",
    "#Hypter parameters which gridsearchcv use for tuning the model\n",
    "param_grid = {'doc2vec__window': [3, 6, 9], # window size of doc2vec model\n",
    "              'doc2vec__vector_size': [300,500], # final vector size of each document\n",
    "              'svc__kernel':['linear','rbf'], # different kernels used by svc\n",
    "              'svc__C':[0.3, 0.6, 0.9] # regularization for svc\n",
    "                        \n",
    "}\n",
    "\n",
    "#gridsearchcv pipeline for Doc2vec and SVC\n",
    "pipe_svc = Pipeline([('doc2vec', Doc2Vec_Model()), ('svc', SVC(kernel='linear'))])\n",
    "\n",
    "# \n",
    "svc_grid = GridSearchCV(pipe_svc,  # \n",
    "                        param_grid=param_grid,\n",
    "                        scoring=\"accuracy\",\n",
    "                        verbose=10, \n",
    "                        cv = 2)\n",
    "\n",
    "result_tag =  {\"male\":1, \"female\":0} # mapping the gender with binary labels\n",
    "train = []\n",
    "\n",
    "# TaggedDocument representation of all the train data is added to the list \n",
    "# since this representation is used by Doc2vec for model building\n",
    "for index, row in train_data.iterrows(): # \n",
    "    train.append(TaggedDocument(words=row[\"Documents\"], tags=[result_tag.get(row[\"gender\"], -1)]))\n",
    "\n",
    "\n",
    "fitted = svc_grid.fit(train, train_data.gender)\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best Parameters: {}\\n\".format(svc_grid.best_params_))\n",
    "print(\"Best accuracy: {}\\n\".format(svc_grid.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model with best parameters\n",
    "\n",
    "The `final_model` was built based on the best parameters selected from the GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = Doc2Vec_Model(vector_size=500, window=3)\n",
    "#final_model = pickle.load(open(\"doc2vec_model.sav\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = final_model.fit_transform(train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5TxAe-Z7UWWF"
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"test_labels.csv\")\n",
    "test_data = pd.merge(documents_data, test, left_on=\"Author_ID\", right_on=\"id\", copy=False).drop(labels = [\"Author_ID\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VjLeDXMnT_yA"
   },
   "outputs": [],
   "source": [
    "result_tag =  {\"male\":1, \"female\":0}\n",
    "test_labs = []\n",
    "for index, row in test_data.iterrows():\n",
    "    test_labs.append(TaggedDocument(words=row[\"Documents\"], tags=[result_tag.get(row[\"gender\"], -1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=0.6, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_svc = SVC(C=0.6, kernel='rbf')\n",
    "#model_svc = pickle.load(open(\"svc_doc2vec_model.sav\", \"rb\"))\n",
    "model_svc.fit(X_train, train_data.gender)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfroming the test data on the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = final_model.transform(test_labs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train accuracy for SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9848387096774194"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_svc.score(X_train, train_data.gender)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Accuracy for SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_svc.score(X_test, test_data.gender)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As evident from the score above, the SVC model trained on top of Doc2Vec vectors gives us a much better accuracy of 81% than the models we have tried before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the predicted labels for the test data are saved to a `pred_labels.csv` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender = model_svc.predict(X_test)\n",
    "test_data.gender = test_data.gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0UqhqRXiqShu"
   },
   "outputs": [],
   "source": [
    "test_data.drop(labels=[\"Documents\"], inplace=True, axis = 1)\n",
    "test_data.to_csv(\"pred_labels.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled1 (1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
